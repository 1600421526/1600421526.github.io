<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>LOOM-Scope - LOng-cOntext Model evaluation framework</title>
    <meta name="description" content="Providing a convient and comprehensive framework for long-context model evaluation.">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css">
</head>
<body>
    <div class="container">
        <header>
            <h1>LOOM-Scope</h1>
            <p class="subtitle">Providing a convenient and comprehensive framework for long-context model evaluation.</p>            
            <div class="nav-buttons">
                <a href="" class="button">
                    <i class="fas fa-file-alt"></i>
                    Paper
                </a>
                <a href="https://gitlab.com/ZetangForward1/LOOM-Scape" class="button">
                    <i class="fab fa-github"></i>
                    Code
                </a>
                <a href="" class="button">
                    <i class="fas fa-video"></i>
                    Video
                </a>
            </div>
        </header>

        <section class="section">
            <h2><i class="fas fa-info-circle"></i> Introduction</h2>
            <div style="display: flex; flex-direction: column; align-items: center; text-align: center;">
                <div id="sunburst-chart" style="max-width: 100%; width: 800px; height: 600px;">
                    <!-- Sunburst chart will be rendered here -->
                </div>
                <p class="image-caption">
                    Sunburst Chart of LOOM-Scope with all of our benchmarks (classified)
                </p>
            </div>
            
            <p> 
                ðŸ”¥ LOOM-Scope offers a comprehensive suite of tools, featuring 15 standard long-context benchmarks covering faithfulness, retrieval, reasoning, generation, and more. With hundreds of subtasks and support for diverse model architectures including Transformers, Mamba, and RWKV it enables researchers to rigorously assess model capabilities under real-world long-context demands. A standout feature is its single-line command workflow, automating dataset/model detection, download, and evaluation for seamless usability.<br>
                <br>
                ðŸš€ Efficiency is at the core of LOOM-Scope: it integrates 12 computational acceleration methods like CakeKV and FlexPrefill, leveraging vLLM for fast, memory-efficient inference. We have evaluated a range of GPU hardware, from the 3090 with 24GB VRAM to the H20 with 96GB VRAM, and publicly documented their computational costs while ensuring reproducible results across platforms. Whether evaluating base models or adapters (via HuggingFace PEFT), LOOM-Scope ensures consistency and scalability.<br>
            </p>
        </section>

        <section class="section">
            <h2><i class="fas fa-chart-bar"></i> Model Leaderboard</h2>
            <p> 
                The model leaderboard showcases performance rankings across LOOM-Scope's multidimensional benchmarks, evaluating models on key capabilities including Faithfulness, General tasks, Reasoning, Retrieval, Generation, and Specialization.
            </p>
            
            <div class="capability-controls">
                <button class="capability-btn active" data-ability="all">All Capabilities</button>
                <button class="capability-btn" data-ability="faithfulness">Faithfulness</button>
                <button class="capability-btn" data-ability="general">General Tasks</button>
                <button class="capability-btn" data-ability="reasoning">Reasoning</button>
                <button class="capability-btn" data-ability="retrieval">Retrieval</button>
                <button class="capability-btn" data-ability="generation">Generation</button>
                <button class="capability-btn" data-ability="specialization">Specialization</button>
            </div>
            
            <div class="table-container">
                <table id="model-leaderboard">
                    <thead>
                        <tr>
                            <th class="basic-header rank-col">Rank</th>
                            <th class="basic-header model-col">Model</th>
                            <th class="basic-header score-col">Avg Score</th>
                            <th class="algorithm-group" data-ability="faithfulness">Faithfulness</th>
                            <th class="algorithm-group" data-ability="general">General Tasks</th>
                            <th class="algorithm-group" data-ability="reasoning">Reasoning</th>
                            <th class="algorithm-group" data-ability="retrieval">Retrieval</th>
                            <th class="algorithm-group" data-ability="generation">Generation</th>
                            <th class="algorithm-group" data-ability="specialization">Specialization</th>
                        </tr>
                    </thead>
                    <tbody>
                        <!-- Table content will be generated by JavaScript -->
                    </tbody>
                </table>
            </div>
            
            <div class="legend">
                <div class="legend-item">
                    <div class="legend-color legend-qwen"></div>
                    <span>Qwen Model Series</span>
                </div>
                <div class="legend-item">
                    <div class="legend-color legend-llama"></div>
                    <span>Llama Model Series</span>
                </div>
            </div>
        </section>
 
        <section class="section">
            <h2><i class="fa-solid fa-flask"></i> Benchmark Validation Platform</h2>
            <p>
                The Benchmark Validation Platform provides a side-by-side comparison of scores from our frameworkâ€™s evaluations and official scores reported in benchmark papers, demonstrating research findingsâ€™ reproducibility.
            </p>
            <div class="table-container">
                <table>
                    <thead>
                        <tr>
                            <th rowspan="3" class="basic-header rank-col">Ability</th>
                            <th rowspan="3" class="basic-header rank-col">Benchmark</th>
                            <th rowspan="3" class="basic-header rank-col">Model</th>
                            <th rowspan="3" class="basic-header model-col">Scores (Evaluated)</th>
                            <th rowspan="3" class="basic-header score-col">Official Scores (Reported in Paper)</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td rowspan="5">Reasoning</td>
                            <td>babilong</td>
                            <td><span class="model-label Llama">Llama-3.1-8B-Instruct</span></td>
                            <td>57.0</td>
                            <td>59.0</td>
                        </tr>
                        <tr>
                            <td>Counting_Stars</td>
                            <td><span class="model-label Llama">Llama-3.1-8B-Instruct</span></td>
                            <td>38.2</td>
                            <td>43.5</td>
                        </tr>
                        <tr>
                            <td>LongBench_v2</td>
                            <td><span class="model-label Llama">Llama-3.1-8B-Instruct</span></td>
                            <td>30.4</td>
                            <td>30.0</td>
                        </tr>
                        <tr>
                            <td>LongIns</td>
                            <td><span class="model-label Qwen">ChatGLM2-6B</span></td>
                            <td>11.5</td>
                            <td>12.0</td>
                        </tr>
                        <tr>
                            <td>LVEval</td>
                            <td><span class="model-label Llama">Llama2-7B-32k-Instruct</span></td>
                            <td>7.3</td>
                            <td>7.4</td>
                        </tr>
                        <tr>
                            <td rowspan="4">General</td>
                            <td>LEval</td>
                            <td><span class="model-label Llama">Llama-3.1-8B-Instruct</span></td>
                            <td>60.5</td>
                            <td>58.7</td>
                        </tr>
                        <tr>
                            <td>LongBench</td>
                            <td><span class="model-label Qwen">ChatGLM2-6B</span></td>
                            <td>24.9</td>
                            <td>25.7</td>
                        </tr>
                        <tr>
                            <td>LooGLE</td>
                            <td><span class="model-label Qwen">ChatGLM2-6b-32k</span></td>
                            <td>19.6</td>
                            <td>15.1</td>
                        </tr>
                        <tr>
                            <td>RULER</td>
                            <td><span class="model-label Llama">Llama-3.1-8B-Instruct</span></td>
                            <td>90.7</td>
                            <td>88.3</td>
                        </tr>
                        <tr>
                            <td rowspan="3">Retrieval</td>
                            <td>InfiniteBench</td>
                            <td><span class="model-label Qwen">ChatGLM3-6b-128k</span></td>
                            <td>24.5</td>
                            <td>19.5</td>
                        </tr>
                        <tr>
                            <td>NIAH</td>
                            <td><span class="model-label Llama">Llama-3.1-8B-Instruct</span></td>
                            <td>97.6</td>
                            <td>-</td>
                        </tr>
                        <tr>
                            <td>NThread</td>
                            <td><span class="model-label Llama">Llama-3.1-8B-Instruct</span></td>
                            <td>34.3</td>
                            <td>41.4</td>
                        </tr>
                        <tr>
                            <td>Generation</td>
                            <td>LongWriter</td>
                            <td><span class="model-label Llama">Llama-3.1-8B-Instruct</span></td>
                            <td>58.5</td>
                            <td>60.3</td>
                        </tr>
                        <tr>
                            <td>Specialization</td>
                            <td>LIBRA</td>
                            <td><span class="model-label Llama">Llama-3-8B-Instruct</span></td>
                            <td>56.8</td>
                            <td>57.4</td>
                        </tr>
                        <tr>
                            <td>Faithfulness</td>
                            <td>L_CiteEval</td>
                            <td><span class="model-label Llama">Llama-3.1-8B-Instruct</span></td>
                            <td>27.7</td>
                            <td>25.5</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </section>
        
        <section class="section">
            <h2><i class="fas fa-quote-right"></i> BibTeX</h2>
            <div class="citation-container">
                <button class="copy-button" onclick="copyBibTeX()">
                    <i class="fas fa-copy"></i> Copy
                </button>
                <pre id="bibtex">@misc{loomscope2024,
    title={LOOM-Scope: LOng-cOntext Model evaluation framework}, 
    author={Zetang Forward Team},
    year={2024},
    eprint={2504.19093},
    archivePrefix={arXiv},
    primaryClass={cs.CL},
    url={https://arxiv.org/abs/2504.19093}, 
}</pre>
            </div>
        </section>
    </div>

    <script>
        // Model performance data
        const scores = {
            "Qwen3-30B-A3B": [37.96,40.61,11.61,78.32,43.24,60.31,48.96,41.30,22.82,28.42,100.00,24.12,14.14,83.24,56.09],
            "Qwen3-14B": [35.64,43.84,11.79,74.94,45.47,59.15,56.41,31.95,21.26,29.85,100.00,27.35,10.24,85.75,55.87],
            "Meta-Llama-3.1-8B-Instruct": [5.79,39.70,11.81,86.79,37.94,57.42,37.68,25.40,25.66,30.40,91.00,20.06,33.64,45.96,51.24],
            "Qwen3-8B": [33.18,41.15,11.67,67.68,38.62,55.28,52.32,32.61,15.15,27.25,64.00,21.92,8.06,81.99,51.78],
            "Qwen3-4B": [24.55,39.03,11.69,70.29,39.32,55.01,42.06,33.66,18.24,32.52,62.00,17.95,13.05,74.25,46.92]
        };
        
        // Capability definitions and tests
        const capabilities = {
            "faithfulness": {
                "tests": ["L_CiteEval"],
                "description": "Evaluates model consistency with source information, avoiding hallucinations or inaccuracies."
            },
            "general": {
                "tests": ["LEval", "LooGLE", "RULER(0-128k)", "longbench"],
                "description": "Measures performance on common NLP tasks including text understanding and classification."
            },
            "reasoning": {
                "tests": ["Counting-Stars", "LongIns", "LVEval", "longbench_v2", "NIAH"],
                "description": "Tests logical reasoning, mathematical computation, and complex problem-solving abilities."
            },
            "retrieval": {
                "tests": ["NThread", "InfiniteBench", "LongWriter"],
                "description": "Assesses ability to locate and extract relevant information from large contexts."
            },
            "generation": {
                "tests": ["LongWriter"],
                "description": "Evaluates coherent, contextually appropriate, and creative text generation."
            },
            "specialization": {
                "tests": ["LIBRA"],
                "description": "Measures expertise in specialized domains like law, medicine, and programming."
            }
        };
        
        // Capability weightings
        const abilitySize = {
            "faithfulness": 1,
            "general": 4,
            "reasoning": 5,
            "retrieval": 3,
            "generation": 1,
            "specialization": 1
        };
        
        // Test sequence
        const sequence = [
            "L_CiteEval", "LEval", "LooGLE", "RULER(0-128k)", "longbench", 
            "babilong(0-128k)", "Counting-Stars", "LongIns", "LVEval", 
            "longbench_v2", "NIAH", "NThread", "InfiniteBench", "LongWriter", "LIBRA"
        ];
        
        // Calculate capability averages
        const calculateAbilityAverage = (modelScores, abilityName) => {
            const tests = capabilities[abilityName].tests;
            const testIndices = tests.map(test => sequence.indexOf(test));
            
            let sum = 0;
            for (const index of testIndices) {
                sum += modelScores[index];
            }
            return sum / tests.length;
        };
        
        // Prepare model data with calculated metrics
        const prepareModelData = () => {
            return Object.keys(scores).map(model => {
                const modelScores = scores[model];
                const avgScore = modelScores.reduce((sum, score) => sum + score, 0) / modelScores.length;
                
                return {
                    name: model,
                    scores: modelScores,
                    avgScore: avgScore,
                    faithScore: calculateAbilityAverage(modelScores, "faithfulness"),
                    generalScore: calculateAbilityAverage(modelScores, "general"),
                    reasoningScore: calculateAbilityAverage(modelScores, "reasoning"),
                    retrievalScore: calculateAbilityAverage(modelScores, "retrieval"),
                    generationScore: calculateAbilityAverage(modelScores, "generation"),
                    specializationScore: calculateAbilityAverage(modelScores, "specialization")
                };
            });
        };
        
        // Generate table row for each model
        const generateModelRow = (model, rank) => {
            const isQwen = model.name.includes("Qwen");
            const modelClass = isQwen ? "Qwen" : "Llama";
            
            return `<tr>
                <td class="rank-col">${rank}</td>
                <td class="model-col"><span class="model-label ${modelClass}">${model.name}</span></td>
                <td class="best-score">${model.avgScore.toFixed(2)}</td>
                <td>${model.faithScore.toFixed(2)}</td>
                <td>${model.generalScore.toFixed(2)}</td>
                <td>${model.reasoningScore.toFixed(2)}</td>
                <td>${model.retrievalScore.toFixed(2)}</td>
                <td>${model.generationScore.toFixed(2)}</td>
                <td>${model.specializationScore.toFixed(2)}</td>
            </tr>`;
        };
        
        // Render the leaderboard table
        const renderTable = (models) => {
            const tbody = document.querySelector("#model-leaderboard tbody");
            tbody.innerHTML = "";
            
            models.forEach((model, index) => {
                tbody.innerHTML += generateModelRow(model, index + 1);
            });
        };
        
        // Sort table by capability
        const sortTable = (ability, direction = "desc") => {
            let models = prepareModelData();
            
            // Update sort indicators
            document.querySelectorAll("th[data-ability]").forEach(th => {
                th.classList.remove("sorted-asc", "sorted-desc");
            });
            
            // Add current sort indicator
            const header = document.querySelector(`th[data-ability="${ability}"]`);
            if (header) {
                header.classList.add(direction === "asc" ? "sorted-asc" : "sorted-desc");
            }
            
            // Sort models
            models.sort((a, b) => {
                const aScore = a[`${ability}Score`];
                const bScore = b[`${ability}Score`];
                
                if (direction === "asc") {
                    return aScore - bScore;
                } else {
                    return bScore - aScore;
                }
            });
            
            renderTable(models);
        };
        
        // Initialize event listeners
        const initEventListeners = () => {
            // Capability header sorting
            document.querySelectorAll("th[data-ability]").forEach(th => {
                th.addEventListener("click", (e) => {
                    const ability = e.target.dataset.ability;
                    const currentDirection = e.target.classList.contains("sorted-asc") ? "desc" : "asc";
                    sortTable(ability, currentDirection);
                });
            });
            
            // Capability filter buttons
            document.querySelectorAll(".capability-btn").forEach(btn => {
                btn.addEventListener("click", (e) => {
                    const ability = e.target.dataset.ability;
                    
                    // Update active button
                    document.querySelectorAll(".capability-btn").forEach(b => {
                        b.classList.remove("active");
                    });
                    e.target.classList.add("active");
                    
                    // If a specific capability is selected, sort by that capability
                    if (ability !== "all") {
                        sortTable(ability, "desc");
                    } else {
                        // For "all", sort by average score
                        sortTable("avg", "desc");
                    }
                });
            });
        };
        
        // Copy BibTeX function
        function copyBibTeX() {
            const bibtex = document.getElementById('bibtex').textContent;
            navigator.clipboard.writeText(bibtex).then(() => {
                const button = document.querySelector('.copy-button');
                const originalContent = button.innerHTML;
                button.innerHTML = '<i class="fas fa-check"></i> Copied!';
                setTimeout(() => {
                    button.innerHTML = originalContent;
                }, 2000);
            });
        }
        
        // Initialize on page load
        window.onload = () => {
            initEventListeners();
            
            // Initial render (sorted by average score)
            let models = prepareModelData();
            models.sort((a, b) => b.avgScore - a.avgScore);
            renderTable(models);
            
            // Set initial sort indicator for average score
            document.querySelector(".score-col").classList.add("sorted-desc");
        };
    </script>
</body>
</html>