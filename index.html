<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>LOOM-Scope - LOng-cOntext Model evaluation framework</title>
    <meta name="description" content="Providing a convient and comprehensive framework for long-context model evaluation.">
    <link rel="stylesheet" href="styles.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css">
    
</head>
<body>
    <div class="container">
        <header>
            <h1>LOOM-Scope</h1>
            <p class="subtitle">Providing a convenient and comprehensive framework for long-context model evaluation.</p>            
            <div class="nav-buttons">
                <a href="" class="button">
                    <i class="fas fa-file-alt"></i>
                    Paper
                </a>
                <a href="https://gitlab.com/ZetangForward1/LOOM-Scape" class="button">
                    <i class="fab fa-github"></i>
                    Code
                </a>
                <a href="" class="button">
                    <i class="fas fa-video"></i>
                    Video
                </a>
            </div>
        </header>
        <section class="section">
            <h2><i class="fas fa-info-circle"></i> Introduction</h2>
            <div style="display: flex; flex-direction: column; align-items: center; text-align: center;">
                <div id="sunburst-chart" style="max-width: 100%; width: 800px; height: 600px;">
                    <!-- Sunburst chart will be rendered here -->
                </div>
                <p class="image-caption">
                    Sunburst Chart of LOOM-Scope with all of our benchmarks (classified)
                </p>
            </div>
            
            <p> 
                🔥 LOOM-Scope offers a comprehensive suite of tools, featuring 15 standard long-context benchmarks covering faithfulness, retrieval, reasoning, generation, and more. With hundreds of subtasks and support for diverse model architectures including Transformers, Mamba, and RWKV it enables researchers to rigorously assess model capabilities under real-world long-context demands. A standout feature is its single-line command workflow, automating dataset/model detection, download, and evaluation for seamless usability.<br>
                <br>
                🚀 Efficiency is at the core of LOOM-Scope: it integrates 12 computational acceleration methods like CakeKV and FlexPrefill, leveraging vLLM for fast, memory-efficient inference. We have evaluated a range of GPU hardware, from the 3090 with 24GB VRAM to the H20 with 96GB VRAM, and publicly documented their computational costs while ensuring reproducible results across platforms. Whether evaluating base models or adapters (via HuggingFace PEFT), LOOM-Scope ensures consistency and scalability.<br>
            </p>
        </section>

        <!-- 替换的交互式模型排行榜部分 -->
        <section class="section evaluation">
            <h2><i class="fas fa-chart-bar"></i> Model leaderboard</h2>
            <p> 
                The model leaderboard showcases performance rankings across LOOM-Scope's multidimensional benchmarks, evaluating models on key capabilities including Faithfulness, General tasks, Reasoning, Retrieval, Generation, and Specialization.
            </p> 
            
            <div class="capability-controls">
                <button class="capability-btn active" data-capability="overview">Overview</button>
                <button class="capability-btn" data-capability="faithfulness">Faithfulness</button>
                <button class="capability-btn" data-capability="general">General</button>
                <button class="capability-btn" data-capability="reasoning">Reasoning</button>
                <button class="capability-btn" data-capability="retrieval">Retrieval</button>
                <button class="capability-btn" data-capability="generation">Generation</button>
                <button class="capability-btn" data-capability="specialization">Specialization</button>
            </div>
            
            <div class="capability-info" id="capability-info">
                <h2><i class="fas fa-info-circle"></i> <span id="info-title">Model Performance Overview</span></h2>
                <p id="info-description">Overall model performance across all capabilities and benchmark tests.</p>
            </div>
            
            <div class="table-container">
                <table id="model-leaderboard">
                    <thead id="table-header-row">
                        <!-- Header will be generated dynamically -->
                    </thead>
                    <tbody>
                        <!-- Table content will be generated by JavaScript -->
                    </tbody>
                </table>
            </div>
            
            <div class="legend">
                <div class="legend-item">
                    <div class="legend-color legend-qwen"></div>
                    <span>Qwen Model Series</span>
                </div>
                <div class="legend-item">
                    <div class="legend-color legend-llama"></div>
                    <span>Llama Model Series</span>
                </div>
            </div>
        </section>
 
        
        <section class="section">
            <h2><i class="fa-solid fa-flask"></i> Benchmark Validation Platform</h2>
            <p>
                The Benchmark Validation Platform provides a side-by-side comparison of scores from our framework’s evaluations and official scores reported in benchmark papers, demonstrating research findings’ reproducibility.
            </p>
            <div class="table-container">
                <table>
                    <thead>
                        <tr>
                            <th rowspan="3" class="basic-header rank-col">Ability</th>
                            <th rowspan="3" class="basic-header rank-col">Benchmark</th>
                            <th rowspan="3" class="basic-header rank-col">Model</th>
                            <th rowspan="3" class="basic-header model-col">Scores (Evaluated)</th>
                            <th rowspan="3" class="basic-header score-col">Official Scores (Reported in Paper)</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td rowspan="5">Reasoning</td>
                            <td>babilong</td>
                            <td><span class="model-label Retrieval">Llama-3.1-8B-Instruct</span></td>
                            <td>57.0</td>
                            <td>59.0</td>
                        </tr>
                        <tr>
                            <td>Counting_Stars</td>
                            <td><span class="model-label Retrieval">Llama-3.1-8B-Instruct</span></td>
                            <td>38.2</td>
                            <td>43.5</td>
                        </tr>
                        <tr>
                            <td>LongBench_v2</td>
                            <td><span class="model-label Retrieval">Llama-3.1-8B-Instruct</span></td>
                            <td>30.4</td>
                            <td>30.0</td>
                        </tr>
                        <tr>
                            <td>LongIns</td>
                            <td><span class="model-label Specialization">ChatGLM2-6B</span></td>
                            <td>11.5</td>
                            <td>12.0</td>
                        </tr>
                        <tr>
                            <td>LVEval</td>
                            <td><span class="model-label Retrieval">Llama2-7B-32k-Instruct</span></td>
                            <td>7.3</td>
                            <td>7.4</td>
                        </tr>
                        <tr>
                            <td rowspan="4">General</td>
                            <td>LEval</td>
                            <td><span class="model-label Retrieval">Llama-3.1-8B-Instruct</span></td>
                            <td>60.5</td>
                            <td>58.7</td>
                        </tr>
                        <tr>
                            <td>LongBench</td>
                            <td><span class="model-label Specialization">ChatGLM2-6B</span></td>
                            <td>24.9</td>
                            <td>25.7</td>
                        </tr>
                        <tr>
                            <td>LooGLE</td>
                            <td><span class="model-label Specialization">ChatGLM2-6b-32k</span></td>
                            <td>19.6</td>
                            <td>15.1</td>
                        </tr>
                        <tr>
                            <td>RULER</td>
                            <td><span class="model-label Retrieval">Llama-3.1-8B-Instruct</span></td>
                            <td>90.7</td>
                            <td>88.3</td>
                        </tr>
                        <tr>
                            <td rowspan="3">Retrieval</td>
                            <td>InfiniteBench</td>
                            <td><span class="model-label Specialization">ChatGLM3-6b-128k</span></td>
                            <td>24.5</td>
                            <td>19.5</td>
                        </tr>
                        <tr>
                            <td>NIAH</td>
                            <td><span class="model-label Retrieval">Llama-3.1-8B-Instruct</span></td>
                            <td>97.6</td>
                            <td>-</td>
                        </tr>
                        <tr>
                            <td>NThread</td>
                            <td><span class="model-label Retrieval">Llama-3.1-8B-Instruct</span></td>
                            <td>34.3</td>
                            <td>41.4</td>
                        </tr>
                        <tr>
                            <td>Generation</td>
                            <td>LongWriter</td>
                            <td><span class="model-label Retrieval">Llama-3.1-8B-Instruct</span></td>
                            <td>58.5</td>
                            <td>60.3</td>
                        </tr>
                        <tr>
                            <td>Specialization</td>
                            <td>LIBRA</td>
                            <td><span class="model-label Retrieval">Llama-3-8B-Instruct</span></td>
                            <td>56.8</td>
                            <td>57.4</td>
                        </tr>
                        <tr>
                            <td>Faithfulness</td>
                            <td>L_CiteEval</td>
                            <td><span class="model-label Retrieval">Llama-3.1-8B-Instruct</span></td>
                            <td>27.7</td>
                            <td>25.5</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </section>
        <section class="section">
            <h2><i class="fas fa-quote-right"></i> BibTeX</h2>
            <div class="citation-container">
                <button class="copy-button" onclick="copyBibTeX()">
                    <i class="fas fa-copy"></i> Copy
                </button>
                <pre id="bibtex">@misc{loomscope2024,
    title={LOOM-Scope: LOng-cOntext Model evaluation framework}, 
    author={Zetang Forward Team},
    year={2024},
    eprint={2504.19093},
    archivePrefix={arXiv},
    primaryClass={cs.CL},
    url={https://arxiv.org/abs/2504.19093}, 
}</pre>
            </div>
        </section>
    </div>

    <!-- 太阳图脚本 -->
    <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
    <script>
        // 定义标签（labels）和父节点（parents）
        const labels = ["Synthetic Tasks", "Real-World Tasks", "S-Doc QA", "M-Doc QA", "Choose", "Long QA", "Summ", "Writing", " Synthetic Recall ", "Synthetic Recall", "Code & Math", "Code & Math ", "Cite", "ICL", " RULER", " LongIns", "LEval", "LVEval", " LongBench-V2 ", " RULER ", "Babilong ", " LongIns ", " NThread ", " Babilong", "LongIns  ", "L-Cite-Eval ", "LEval ", "LongBench ", "LooGLE ", "InfiniteBench ", " LEval", "LongWriter", " LEval ", "LVEval ", " LongBench", " LooGLE", " LIBRA", " L-Cite-Eval", "LongBench-V2", " InfiniteBench", "LIBRA ", " LongBench ", "LongBench-V2 ", "LEval  ", " LVEval ", " LongBench-V2", "RULER", "Babilong", " InfiniteBench ", "LongIns", "NThread", "L-Cite-Eval  ", "  LEval", "LVEval  ", "LongBench  ", "RULER ", "InfiniteBench  ", "LongIns ", "Counting-Stars", "   NIAH   ", "NThread ", " LIBRA ", "  LEval  ", "  LVEval  ", "  InfiniteBench  "]
        const parents = ["", "", "Synthetic Tasks", "Synthetic Tasks", "Synthetic Tasks", "Real-World Tasks", "Real-World Tasks", "Real-World Tasks", "Synthetic Tasks", "Real-World Tasks", "Synthetic Tasks", "Real-World Tasks", "Real-World Tasks", "Real-World Tasks", "S-Doc QA", "S-Doc QA", "M-Doc QA", "M-Doc QA", "M-Doc QA", "M-Doc QA", "M-Doc QA", "M-Doc QA", "M-Doc QA", "Choose", "Choose", "Summ", "Summ", "Summ", "Summ", "Summ", "Writing", "Writing", "Synthetic Recall", "Synthetic Recall", "Synthetic Recall", "Synthetic Recall", "Synthetic Recall", "Cite", "Code & Math ", "Code & Math ", "Code & Math ", "ICL", "ICL", "Long QA", "Long QA", "Long QA", "Long QA", "Long QA", "Long QA", "Long QA", "Long QA", " Synthetic Recall ", " Synthetic Recall ", " Synthetic Recall ", " Synthetic Recall ", " Synthetic Recall ", " Synthetic Recall ", " Synthetic Recall ", " Synthetic Recall ", " Synthetic Recall ", " Synthetic Recall ", " Synthetic Recall ", "Code & Math", "Code & Math", "Code & Math"];
        const values = [650, 650, 52, 182, 52, 200, 125, 50, 286, 125, 78, 75, 25, 50, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26];
        const x = 14 + 11 + 5 + 2 + 5 + 1 + 3 + 2 + 8 + 11 + 3;
        const labels_trimmed = labels.slice(0, x);
        const parents_trimmed = parents.slice(0, x);
        const values_trimmed = values.slice(0, x);
    
        // 定义颜色
        const colors = [];
        const main_colors = ["#1f77b4", "#ff7f0e"]; // 主类别颜色
        const secondary_colors = ["#aec7e8", "#ffbb78"]; // 二级类别颜色
        const tertiary_colors = ["#c6dbef", "#fdd0a2"]; // 三级类别颜色
    
        for (let i = 0; i < labels_trimmed.length; i++) {
            const label = labels_trimmed[i];
            const parent = parents_trimmed[i];
            if (parent === "") {
                colors.push(main_colors[labels_trimmed.indexOf(label)]);
            } else if (parent === "Synthetic Tasks" || parent === "Real-World Tasks") {
                const index = labels_trimmed.indexOf(parent) % 2;
                colors.push(secondary_colors[index]);
            } else {
                const index = labels_trimmed.indexOf(parent) % 2;
                if (parent === "Summ" || parent === "Cite") {
                    colors.push(tertiary_colors[1]);
                } else if (parent === "M-Doc QA") {
                    colors.push(tertiary_colors[0]);
                } else {
                    colors.push(tertiary_colors[index]);
                }
            }
        }
    
        const fig = {
            data: [{
                type: 'sunburst',
                labels: labels_trimmed,
                parents: parents_trimmed,
                values: values_trimmed,
                branchvalues: "total",
                maxdepth: 3,
                marker: {
                    colors: colors,
                    line: {
                        color: 'white',
                        width: 0.5
                    }
                },
                hovertemplate: '<b>%{label}</b><br>Value: %{value}<extra></extra>'
            }],
            layout: {
                margin: { t: 20, l: 20, r: 20, b: 20 },
                font: {
                    size: 12,
                    color: "black"
                },
                autosize: false,
                width: 800,
                height: 600,
                paper_bgcolor: "transparent", // 画布背景透明
                plot_bgcolor: "transparent"  // 绘图区域背景透明
            }
        };
    
        // 渲染图表到指定的 div 元素中
        Plotly.newPlot('sunburst-chart', fig);
    </script>

    <!-- 交互式模型排行榜脚本 -->
    <script>
        // Model performance data
        const scores = {
            "Qwen3-30B-A3B": [37.96,40.61,11.61,78.32,43.24,60.31,48.96,41.30,22.82,28.42,100.00,24.12,14.14,83.24,56.09],
            "Qwen3-14B": [35.64,43.84,11.79,74.94,45.47,59.15,56.41,31.95,21.26,29.85,100.00,27.35,10.24,85.75,55.87],
            "Meta-Llama-3.1-8B-Instruct": [25.79,39.70,11.81,86.79,37.94,57.42,37.68,25.40,25.66,30.40,91.00,20.06,33.64,45.96,51.24],
            "Qwen3-8B": [33.18,41.15,11.67,67.68,38.62,55.28,52.32,32.61,15.15,27.25,64.00,21.92,8.06,81.99,51.78],
            "Qwen3-4B": [24.55,39.03,11.69,70.29,39.32,55.01,42.06,33.66,18.24,32.52,62.00,17.95,13.05,74.25,46.92]
        };
        const capabilities = {
            "overview": {
                "description": "Overall model performance across all capabilities and benchmark tests",
                "columns": [
                    { name: "Rank", type: "fixed", sortable: true },
                    { name: "Model", type: "fixed", sortable: false },
                    { name: "Avg Score", type: "fixed", sortable: true },
                    { name: "Faithfulness", type: "capability", sortable: true },
                    { name: "General", type: "capability", sortable: true },
                    { name: "Reasoning", type: "capability", sortable: true },
                    { name: "Retrieval", type: "capability", sortable: true },
                    { name: "Generation", type: "capability", sortable: true },
                    { name: "Specialization", type: "capability", sortable: true }
                ]
            },
            "faithfulness": {
                "tests": ["L_CiteEval"],
                "description": "Evaluates model consistency with source information, avoiding hallucinations or inaccuracies. Measures how well models can produce factually accurate outputs based on given inputs.",
                "columns": [
                    { name: "Rank", type: "fixed", sortable: true },
                    { name: "Model", type: "fixed", sortable: false },
                    { name: "Avg Score", type: "fixed", sortable: true },
                    { name: "L_CiteEval", type: "test", sortable: true }
                ]
            },
            "general": {
                "tests": ["LEval", "LooGLE", "RULER", "longbench"],
                "description": "Measures performance on common NLP tasks including text understanding, classification, and information extraction across diverse domains and contexts.",
                "columns": [
                    { name: "Rank", type: "fixed", sortable: true },
                    { name: "Model", type: "fixed", sortable: false },
                    { name: "Avg Score", type: "fixed", sortable: true },
                    { name: "LEval", type: "test", sortable: true },
                    { name: "LooGLE", type: "test", sortable: true },
                    { name: "RULER", type: "test", sortable: true },
                    { name: "longbench", type: "test", sortable: true }
                ]
            },
            "reasoning": {
                "tests": ["Counting_Stars", "LongIns", "LVEval", "longbench_v2", "NIAH","babilong"],
                "description": "Tests logical reasoning, mathematical computation, causal inference and complex problem-solving abilities. Evaluates multi-step reasoning and abstract thinking capabilities.",
                "columns": [
                    { name: "Rank", type: "fixed", sortable: true },
                    { name: "Model", type: "fixed", sortable: false },
                    { name: "Avg Score", type: "fixed", sortable: true },
                    { name: "Counting-Stars", type: "test", sortable: true },
                    { name: "LongIns", type: "test", sortable: true },
                    { name: "LVEval", type: "test", sortable: true },
                    { name: "longbench_v2", type: "test", sortable: true },
                    { name: "NIAH", type: "test", sortable: true },
                    { name: "babilong", type: "test", sortable: true }
                ]
            },
            "retrieval": {
                "tests": ["NThread", "InfiniteBench", "LongWriter"],
                "description": "Assesses ability to locate, extract and synthesize relevant information from large contexts. Measures performance in long-document processing and information localization.",
                "columns": [
                    { name: "Rank", type: "fixed", sortable: true },
                    { name: "Model", type: "fixed", sortable: false },
                    { name: "Avg Score", type: "fixed", sortable: true },
                    { name: "NThread", type: "test", sortable: true },
                    { name: "InfiniteBench", type: "test", sortable: true },
                    { name: "LongWriter", type: "test", sortable: true }
                ]
            },
            "generation": {
                "tests": ["LongWriter"],
                "description": "Evaluates coherent, contextually appropriate, and creative text generation. Measures fluency, coherence, and creativity in content creation tasks.",
                "columns": [
                    { name: "Rank", type: "fixed", sortable: true },
                    { name: "Model", type: "fixed", sortable: false },
                    { name: "Avg Score", type: "fixed", sortable: true },
                    { name: "LongWriter", type: "test", sortable: true }
                ]
            },
            "specialization": {
                "tests": ["LIBRA"],
                "description": "Measures expertise in specialized domains like law, medicine, programming and finance. Tests domain-specific knowledge and problem-solving abilities.",
                "columns": [
                    { name: "Rank", type: "fixed", sortable: true },
                    { name: "Model", type: "fixed", sortable: false },
                    { name: "Avg Score", type: "fixed", sortable: true },
                    { name: "LIBRA", type: "test", sortable: true }
                ]
            }
        };
        
        const sequence = [
            "L_CiteEval", "LEval", "LooGLE", "RULER", "longbench", 
            "babilong", "Counting_Stars", "LongIns", "LVEval", 
            "longbench_v2", "NIAH", "NThread", "InfiniteBench", "LongWriter", "LIBRA"
        ];
        
        // State variables
        let currentCapability = "overview";
        let sortColumn = "Avg Score";
        let sortDirection = "desc";
        let models = [];
        
        // Calculate capability averages
        const calculateAbilityAverage = (modelScores, tests) => {
            const testIndices = tests.map(test => sequence.indexOf(test));
            
            let sum = 0;
            let count = 0;
            for (const index of testIndices) {
                if (index !== -1) {
                    sum += modelScores[index];
                    count++;
                }
            }
            return count > 0 ? sum / count : 0;
        };
        
        // Prepare model data with calculated metrics
        const prepareModelData = () => {
            return Object.keys(scores).map(model => {
                const modelScores = scores[model];
                const avgScore = modelScores.reduce((sum, score) => sum + score, 0) / modelScores.length;
                
                return {
                    name: model,
                    scores: modelScores,
                    avgScore: avgScore,
                    faithScore: calculateAbilityAverage(modelScores, capabilities.faithfulness.tests),
                    generalScore: calculateAbilityAverage(modelScores, capabilities.general.tests),
                    reasoningScore: calculateAbilityAverage(modelScores, capabilities.reasoning.tests),
                    retrievalScore: calculateAbilityAverage(modelScores, capabilities.retrieval.tests),
                    generationScore: calculateAbilityAverage(modelScores, capabilities.generation.tests),
                    specializationScore: calculateAbilityAverage(modelScores, capabilities.specialization.tests)
                };
            });
        };
        
        // Render table header based on current capability
        const renderTableHeader = () => {
            const headerRow = document.getElementById("table-header-row");
            headerRow.innerHTML = "";
            
            const capability = capabilities[currentCapability];
            
            capability.columns.forEach(col => {
                const th = document.createElement("th");
                th.textContent = col.name;
                
                if (col.sortable) {
                    th.classList.add("sortable");
                    
                    // Add sort indicator if this is the current sort column
                    if (col.name === sortColumn) {
                        th.classList.add(sortDirection === "asc" ? "sorted-asc" : "sorted-desc");
                    }
                    
                    th.addEventListener("click", () => {
                        if (col.sortable) {
                            // Toggle direction if clicking the same column
                            if (sortColumn === col.name) {
                                sortDirection = sortDirection === "asc" ? "desc" : "asc";
                            } else {
                                sortColumn = col.name;
                                sortDirection = "desc";
                            }
                            renderTable();
                        }
                    });
                }
                
                headerRow.appendChild(th);
            });
        };
        
        // Get score for a specific test
        const getTestScore = (model, testName) => {
            // Normalize test names (replace hyphens with underscores)
            const normalizedTestName = testName.replace(/-/g, '_');
            const testIndex = sequence.indexOf(normalizedTestName);
            return testIndex !== -1 ? model.scores[testIndex] : 0;
        };
        
        // Get score for a capability
        const getCapabilityScore = (model, capability) => {
            // 修复能力名大小写问题
            const capabilityName = capability.toLowerCase().replace(/\s+/g, '');
            
            switch(capabilityName) {
                case "faithfulness": return model.faithScore;
                case "general": return model.generalScore;
                case "reasoning": return model.reasoningScore;
                case "retrieval": return model.retrievalScore;
                case "generation": return model.generationScore;
                case "specialization": return model.specializationScore;
                default: return 0;
            }
        };
        
        // Get the appropriate score for the Avg Score column
        const getAvgScore = (model) => {
            if (currentCapability === "overview") {
                return model.avgScore;
            } else {
                // Calculate average of current capability tests
                return calculateAbilityAverage(model.scores, capabilities[currentCapability].tests);
            }
        };
        
        // Render table body
        const renderTableBody = () => {
            const tbody = document.querySelector("#model-leaderboard tbody");
            tbody.innerHTML = "";
            
            // Sort models
            const sortedModels = [...models];
            sortedModels.sort((a, b) => {
                let aValue, bValue;
                
                if (sortColumn === "Rank") {
                    return 0; // Ranking is based on position
                } else if (sortColumn === "Avg Score") {
                    aValue = getAvgScore(a);
                    bValue = getAvgScore(b);
                } else if (currentCapability === "overview") {
                    // 在overview视图中，能力列排序
                    aValue = getCapabilityScore(a, sortColumn);
                    bValue = getCapabilityScore(b, sortColumn);
                } else if (capabilities[currentCapability].tests && 
                           capabilities[currentCapability].tests.includes(sortColumn.replace(/-/g, '_'))) {
                    // 测试列排序
                    aValue = getTestScore(a, sortColumn);
                    bValue = getTestScore(b, sortColumn);
                } else {
                    aValue = 0;
                    bValue = 0;
                }
                
                return sortDirection === "asc" ? aValue - bValue : bValue - aValue;
            });
            
            // Render models
            sortedModels.forEach((model, index) => {
                const isQwen = model.name.includes("Qwen");
                const modelClass = isQwen ? "Qwen" : "Llama";
                
                const row = document.createElement("tr");
                
                capabilities[currentCapability].columns.forEach(col => {
                    const td = document.createElement("td");
                    
                    if (col.name === "Rank") {
                        td.textContent = index + 1;
                        td.classList.add("rank-col");
                    } else if (col.name === "Model") {
                        td.innerHTML = `<span class="model-label ${modelClass}">${model.name}</span>`;
                        td.classList.add("model-col");
                    } else if (col.name === "Avg Score") {
                        td.textContent = getAvgScore(model).toFixed(2);
                        td.classList.add("best-score");
                    } else if (capabilities[currentCapability].tests && 
                               capabilities[currentCapability].tests.includes(col.name.replace(/-/g, '_'))) {
                        // Test score
                        td.textContent = getTestScore(model, col.name).toFixed(2);
                    } else if (currentCapability === "overview") {
                        // Capability score
                        const capabilityName = col.name.toLowerCase().replace(" ", "");
                        td.textContent = getCapabilityScore(model, capabilityName).toFixed(2);
                    }
                    
                    row.appendChild(td);
                });
                
                tbody.appendChild(row);
            });
        };
        
        // Update capability info
        const updateCapabilityInfo = () => {
            const title = document.getElementById("info-title");
            const description = document.getElementById("info-description");
            
            let displayName = currentCapability.charAt(0).toUpperCase() + currentCapability.slice(1);
            if (currentCapability === "general") displayName = "General";
            
            title.textContent = currentCapability === "overview" 
                ? "Model Performance Overview" 
                : displayName + " Capability";
            
            description.textContent = capabilities[currentCapability].description;
        };
        
        // Update capability buttons
        const updateCapabilityButtons = () => {
            document.querySelectorAll(".capability-btn").forEach(btn => {
                if (btn.dataset.capability === currentCapability) {
                    btn.classList.add("active");
                } else {
                    btn.classList.remove("active");
                }
            });
        };
        
        // Render entire table
        const renderTable = () => {
            updateCapabilityInfo();
            renderTableHeader();
            renderTableBody();
            updateCapabilityButtons();
        };
        
        // Initialize event listeners
        const initEventListeners = () => {
            // Capability buttons
            document.querySelectorAll(".capability-btn").forEach(btn => {
                btn.addEventListener("click", (e) => {
                    currentCapability = e.target.dataset.capability;
                    // Reset sorting to default for new view
                    sortColumn = "Avg Score";
                    sortDirection = "desc";
                    renderTable();
                });
            });
        };
        
        // Initialize on page load
        window.onload = () => {
            // Prepare model data
            models = prepareModelData();
            
            // Set default sorting
            models.sort((a, b) => b.avgScore - a.avgScore);
            
            initEventListeners();
            renderTable();
        };
    </script>

    <!-- BibTeX 复制功能 -->
    <script>
        function copyBibTeX() {
            const bibtex = document.getElementById('bibtex').textContent;
            navigator.clipboard.writeText(bibtex).then(() => {
                const button = document.querySelector('.copy-button');
                const originalContent = button.innerHTML;
                button.innerHTML = '<i class="fas fa-check"></i> Copied!';
                setTimeout(() => {
                    button.innerHTML = originalContent;
                }, 2000);
            });
        }
    </script>
</body>
</html>