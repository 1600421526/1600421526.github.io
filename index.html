<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>LOOM-Scope - LOng-cOntext Model evaluation framework</title>
    <meta name="description" content="Providing a convient and comprehensive framework for long-context model evaluation.">
    <link rel="stylesheet" href="styles.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css">
    <style>

        .evaluation {
            overflow-x: auto;
            margin: 20px 0;
            padding: 0 20px;
            width: 100%;
            max-width: 100%;
            -webkit-overflow-scrolling: touch; /* ‰∏∫iOSËÆæÂ§áÊ∑ªÂä†Âπ≥ÊªëÊªöÂä® */
        }

        .evaluation .table-container {
            width: 100%;
            overflow-x: auto;
            margin-bottom: 20px;
        }

        
        .evaluation table {
            border-collapse: collapse;
            min-width: 100%;
            width: max-content;
            margin: 20px auto;
            font-size: 14px;
        }
        
        .evaluation th, .evaluation td {
            padding: 8px 15px;
            text-align: center;
            border: none;
            min-width: 80px;
            white-space: nowrap; /* Èò≤Ê≠¢ÂÜÖÂÆπÊç¢Ë°å */
        }
        
        .evaluation tbody tr {
            border-bottom: 1px solid #ddd;
        }
        
        
        .evaluation thead tr {
            border-bottom: 2px solid #000;
        }
        
        .evaluation th {
            background-color: transparent;
            font-weight: bold;
            white-space: nowrap;
            height: 40px;
            position: sticky; /* Âõ∫ÂÆöË°®Â§¥ */
            top: 0;
            background: white; /* Á°Æ‰øùË°®Â§¥‰∏çÈÄèÊòé */
            z-index: 1;
        }
        
        .evaluation th.rank-col {
            min-width: 60px;
            position: sticky; /* Âõ∫ÂÆöÁ¨¨‰∏ÄÂàó */
            left: 0;
            z-index: 2;
            background: white;
        }
        
        .evaluation th.type-col {
            min-width: 120px;
        }
        
        .evaluation th.model-col {
            min-width: 200px;
        }
        
        .evaluation th.score-col {
            min-width: 100px;
        }
        
        .evaluation th.algorithm-col {
            min-width: 100px;
        }
        
        .evaluation tr:hover {
            background-color: #f5f5f5;
        }
        
c
        
        .Llama {
            background-color: #ffebee;
        }
        
        .model-type-reasoning {
            background-color: #e8f5e9;
        }
        
        .Qwen {
            background-color: #e3f2fd;
        }
        
        .algorithm-group {
            border-left: none;
            padding-left: 15px;
            font-weight: bold;
        }
        
        .model-type-legend {
            display: flex;
            justify-content: center;
            gap: 20px;
            margin: 10px 0;
        }
        
        .legend-item {
            display: flex;
            align-items: center;
            gap: 5px;
        }
        
        .legend-color {
            width: 20px;
            height: 20px;
            border-radius: 4px;
        }

        .best-score {
            font-weight: bold;
        }

        .second-best {
            text-decoration: underline;
        }

        .basic-header {
            height: 60px !important;
        }

        .evaluation::-webkit-scrollbar {
            height: 8px;
        }

        .evaluation::-webkit-scrollbar-track {
            background: #f1f1f1;
            border-radius: 4px;
        }

        .evaluation::-webkit-scrollbar-thumb {
            background: #888;
            border-radius: 4px;
        }

        .evaluation::-webkit-scrollbar-thumb:hover {
            background: #555;
        }
    </style>
</head>
<body>
    <main>
        <section class="hero">
            <div class="logo">
                <i class="fas fa-shield-alt"></i>
            </div>
            <h1>LOOM-Scope</h1>
            <h2>Providing a convient and comprehensive framework for long-context model evaluation.</h2>
            
            <div class="authors">
                <p>
                    <i class="fas fa-users"></i>
                    NONE
                </p>
                <p class="affiliations">
                    <i class="fas fa-university"></i>
                    NONE
                </p>
            </div>
            
            <div class="nav-buttons">
                <a href="" class="button">
                    <i class="fas fa-file-alt"></i>
                    PaperÔºàNONE)
                </a>
                <a href="" class="button">
                    <i class="fab fa-github"></i>
                    CodeÔºàNONE)
                </a>
                <a href="" class="button">
                    <i class="fas fa-database"></i>
                    DataÔºàNONE)
                </a>
            </div>
        </section>

        <section class="introduction">
            <h2><i class="fas fa-info-circle"></i> Introduction</h2>
            <p> üåê LOOM-Scope offers a comprehensive suite of tools, featuring 15 standard long-context benchmarks covering faithfulness, retrieval, reasoning, generation, and more. With hundreds of subtasks and support for diverse model architectures‚Äîincluding Transformers, Mamba, and RWKV‚Äîit enables researchers to rigorously assess model capabilities under real-world long-context demands. A standout feature is its single-line command workflow, automating dataset/model detection, download, and evaluation for seamless usability.<br>

                üöÄ Efficiency is at the core of LOOM-Scope: it integrates 12 computational acceleration methods like CakeKV and FlexPrefill, leveraging vLLM for fast, memory-efficient inference. This makes it compatible with a range of hardware, from 24GB 3090 to 96GB H20 GPUs, with publicly documented computational costs and reproducible results across platforms. Whether evaluating base models or adapters (via HuggingFace PEFT), LOOM-Scope ensures consistency and scalability.<br>
                
                üîÑ By bridging model compatibility, benchmark diversity, and inference optimization, LOOM-Scope empowers the research community to push the boundaries of long-context LLMs. Our evaluation results highlight strengths and gaps in current models, guiding future advancements in context handling, reasoning, and retrieval-augmented generation. Dive into LOOM-Scope to unlock systematic, efficient long-context model evaluation.<br>
                
        </section>

        <section class="evaluation">
            <h2><i class="fas fa-chart-bar"></i> Model leaderboard</h2>
            <div class="table-container">
                <table>
                    <thead>
                        <tr>
                            <th rowspan="3" class="basic-header rank-col">Rank</th>
                            <th rowspan="2" class="basic-header model-col">Model</th>
                            <th rowspan="2" class="basic-header score-col">Avg Score</th>
                            <th colspan="1" class="algorithm-group">Faithfulness</th>
                            <th colspan="4" class="algorithm-group">General</th>
                            <th colspan="5" class="algorithm-group">Reasoning</th>
                            <th colspan="3" class="algorithm-group">Retrieve</th>
                            <th colspan="1" class="algorithm-group">Generation</th>
                            <th colspan="1" class="algorithm-group">Specialization</th>
                        </tr>
                        <tr>
                            <th class="algorithm-col">L_CiteEval</th>
                            <th class="algorithm-col">LEval</th>
                            <th class="algorithm-col">LooGLE</th>
                            <th class="algorithm-col">RULERÔºà0-128kÔºâ</th>
                            <th class="algorithm-col">longbench</th>
                            <th class="algorithm-col">babilongÔºà0-128kÔºâ</th>
                            <th class="algorithm-col">Counting-Stars</th>
                            <th class="algorithm-col">LongIns</th>
                            <th class="algorithm-col">LVEval</th>
                            <th class="algorithm-col">longbench_v2</th>
                            <th class="algorithm-col">NIAH</th>
                            <th class="algorithm-col">NThread</th>
                            <th class="algorithm-col">InfiniteBench</th>
                            <th class="algorithm-col">LongWriter</th>
                            <th class="algorithm-col">LIBRA</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>1</td>
                            <td><span class="model-label Qwen">Qwen3-30B-A3B</span>
                            <td class="best-score">46.08</td> 
                            <td>37.96</td>
                            <td>40.61</td>
                            <td>11.61</td>
                            <td>78.32</td>
                            <td>43.24</td>
                            <td>60.31</td>
                            <td>48.96</td>
                            <td>41.3</td>
                            <td>22.82</td>
                            <td>28.42</td>
                            <td>100</td>
                            <td>24.12</td>
                            <td>14.14</td>
                            <td>83.24</td>
                            <td>56.09</td>
                        </tr>    
                        <tr>
                            <td>2</td>
                            <td><span class="model-label Qwen">Qwen3-14B</span>
                            <td class="best-score">45.97</td>
                            <td>35.64</td>
                            <td>43.84</td>
                            <td>11.79</td>
                            <td>74.94</td>
                            <td>45.47</td>
                            <td>59.15</td>
                            <td>56.41</td>
                            <td>31.95</td>
                            <td>21.26</td>
                            <td>29.85</td>
                            <td>100</td>
                            <td>27.35</td>
                            <td>10.24</td>
                            <td>85.75</td>
                            <td>55.87</td>
                        </tr>
                        <tr>
                            <td>3</td>
                            <td><span class="model-label Llama">Meta-Llama-3.1-8B-Instruct</span></td>
                            <td class="best-score">41.37</td>
                            <td>25.79</td>
                            <td>39.7</td>
                            <td>11.81</td>
                            <td>86.79</td>
                            <td>37.94</td>
                            <td>57.42</td>
                            <td>37.68</td>
                            <td>25.4</td>
                            <td>25.66</td>
                            <td>30.4</td>
                            <td>91</td>
                            <td>20.06</td>
                            <td>33.64</td>
                            <td>45.96</td>
                            <td>51.24</td>
                        </tr>
                        <tr>
                            <td>4</td>
                            <td><span class="model-label Qwen">Qwen3-8B/span>
                            <td class="best-score">40.18</td>
                            <td>33.18</td>
                            <td>41.15</td>
                            <td>11.67</td>
                            <td>67.68</td>
                            <td>38.62</td>
                            <td>55.28</td>
                            <td>52.32</td>
                            <td>32.61</td>
                            <td>15.15</td>
                            <td>27.25</td>
                            <td>64</td>
                            <td>21.92</td>
                            <td>8.06</td>
                            <td>81.99</td>
                            <td>51.78</td>
                        </tr>
                        <tr>
                            <td>5</td>
                            <td><span class="model-label Qwen">Qwen3-4B/span>
                            <td class="best-score">38.70</td>
                            <td>24.55</td>
                            <td>39.03</td>
                            <td>11.69</td>
                            <td>70.29</td>
                            <td>39.32</td>
                            <td>55.01</td>
                            <td>42.06</td>
                            <td>33.66</td>
                            <td>18.24</td>
                            <td>32.52</td>
                            <td>62</td>
                            <td>17.95</td>
                            <td>13.05</td>
                            <td>74.25</td>
                            <td>46.92</td>
                        </tr>    
                    </tbody>
                </table>
            </div>
        </section>
        <section class="introduction">
            <h2><i class="fa-solid fa-database"></i> Benchmark Validation Platform</h2>
            <div class="overview-image">
                <img src="images/newplot.png" alt="LOOM-Scope Overview" class="full-width-image">
                <p class="image-caption"></i> Overview of LOOM-Scope</p>
            </div>

            <!-- L_CiteEval Section -->
            <h2 id="L_CiteEval">1. L_CiteEval</h2>
            <h3>a. Computational Cost (Inference Time)</h3>
            <table class="evaluation-table">
                <thead>
                    <tr>
                        <th>Benchmark</th>
                        <th>Hardware Configuration</th>
                        <th>Inference Time (ALL tasks)</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>L_CiteEval</td>
                        <td>3090 (24GB) √ó 8</td>
                        <td>10:49:25</td>
                    </tr>
                    <tr>
                        <td>L_CiteEval</td>
                        <td>A100 (40GB) √ó 4</td>
                        <td>7:35:33</td>
                    </tr>
                    <tr>
                        <td>L_CiteEval</td>
                        <td>H20 (96GB) √ó 2</td>
                        <td>6:45:38</td>
                    </tr>
                </tbody>
            </table>

            <h3>b. Evaluation Results</h3>
            <table class="evaluation-table">
                <thead>
                    <tr>
                        <th>Benchmark</th>
                        <th>Subtask</th>
                        <th>Metrics</th>
                        <th>Model</th>
                        <th>Results (Evaluated)</th>
                        <th>Official Results (Reported in Paper)</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>L_CiteEval</td>
                        <td>Single Document QA</td>
                        <td>CP</td>
                        <td>Meta-Llama-3.1-8B-Instruct</td>
                        <td>24.2</td>
                        <td>22.7</td>
                    </tr>
                    <tr>
                        <td>L_CiteEval</td>
                        <td>Single Document QA</td>
                        <td>CR</td>
                        <td>Meta-Llama-3.1-8B-Instruct</td>
                        <td>26.4</td>
                        <td>24.7</td>
                    </tr>
                    <tr>
                        <td>L_CiteEval</td>
                        <td>Single Document QA</td>
                        <td>F1</td>
                        <td>Meta-Llama-3.1-8B-Instruct</td>
                        <td>24.9</td>
                        <td>22.6</td>
                    </tr>
                    <tr>
                        <td>L_CiteEval</td>
                        <td>Single Document QA</td>
                        <td>N</td>
                        <td>Meta-Llama-3.1-8B-Instruct</td>
                        <td>2.1</td>
                        <td>2.6</td>
                    </tr>
                    <tr>
                        <td>L_CiteEval</td>
                        <td>Dialogue Understanding</td>
                        <td>CP</td>
                        <td>Meta-Llama-3.1-8B-Instruct</td>
                        <td>52.6</td>
                        <td>51.9</td>
                    </tr>
                    <tr>
                        <td>L_CiteEval</td>
                        <td>Dialogue Understanding</td>
                        <td>CR</td>
                        <td>Meta-Llama-3.1-8B-Instruct</td>
                        <td>57.5</td>
                        <td>57.8</td>
                    </tr>
                    <tr>
                        <td>L_CiteEval</td>
                        <td>Dialogue Understanding</td>
                        <td>F1</td>
                        <td>Meta-Llama-3.1-8B-Instruct</td>
                        <td>53.8</td>
                        <td>53.5</td>
                    </tr>
                    <tr>
                        <td>L_CiteEval</td>
                        <td>Dialogue Understanding</td>
                        <td>N</td>
                        <td>Meta-Llama-3.1-8B-Instruct</td>
                        <td>2.0</td>
                        <td>2.1</td>
                    </tr>
                    <tr>
                        <td>L_CiteEval</td>
                        <td>Needle in a Haystack</td>
                        <td>CP</td>
                        <td>Meta-Llama-3.1-8B-Instruct</td>
                        <td>34.7</td>
                        <td>34.3</td>
                    </tr>
                    <tr>
                        <td>L_CiteEval</td>
                        <td>Needle in a Haystack</td>
                        <td>CR</td>
                        <td>Meta-Llama-3.1-8B-Instruct</td>
                        <td>37.5</td>
                        <td>35.8</td>
                    </tr>
                    <tr>
                        <td>L_CiteEval</td>
                        <td>Needle in a Haystack</td>
                        <td>F1</td>
                        <td>Meta-Llama-3.1-8B-Instruct</td>
                        <td>35.6</td>
                        <td>34.7</td>
                    </tr>
                    <tr>
                        <td>L_CiteEval</td>
                        <td>Needle in a Haystack</td>
                        <td>N</td>
                        <td>Meta-Llama-3.1-8B-Instruct</td>
                        <td>1.2</td>
                        <td>1.0</td>
                    </tr>
                    <tr>
                        <td>L_CiteEval</td>
                        <td>Multi-Doc QA</td>
                        <td>CP</td>
                        <td>Meta-Llama-3.1-8B-Instruct</td>
                        <td>47.6</td>
                        <td>43.41</td>
                    </tr>
                    <tr>
                        <td>L_CiteEval</td>
                        <td>Multi-Doc QA</td>
                        <td>CR</td>
                        <td>Meta-Llama-3.1-8B-Instruct</td>
                        <td>45.6</td>
                        <td>42.0</td>
                    </tr>
                    <tr>
                        <td>L_CiteEval</td>
                        <td>Multi-Doc QA</td>
                        <td>F1</td>
                        <td>Meta-Llama-3.1-8B-Instruct</td>
                        <td>45.6</td>
                        <td>41.6</td>
                    </tr>
                    <tr>
                        <td>L_CiteEval</td>
                        <td>Multi-Doc QA</td>
                        <td>N</td>
                        <td>Meta-Llama-3.1-8B-Instruct</td>
                        <td>1.6</td>
                        <td>1.6</td>
                    </tr>
                    <tr>
                        <td>L_CiteEval</td>
                        <td>Summarization</td>
                        <td>CP</td>
                        <td>Meta-Llama-3.1-8B-Instruct</td>
                        <td>34.0</td>
                        <td>19.6</td>
                    </tr>
                    <tr>
                        <td>L_CiteEval</td>
                        <td>Summarization</td>
                        <td>CR</td>
                        <td>Meta-Llama-3.1-8B-Instruct</td>
                        <td>34.0</td>
                        <td>23.0</td>
                    </tr>
                    <tr>
                        <td>L_CiteEval</td>
                        <td>Summarization</td>
                        <td>F1</td>
                        <td>Meta-Llama-3.1-8B-Instruct</td>
                        <td>31.3</td>
                        <td>20.8</td>
                    </tr>
                    <tr>
                        <td>L_CiteEval</td>
                        <td>Summarization</td>
                        <td>N</td>
                        <td>Meta-Llama-3.1-8B-Instruct</td>
                        <td>19.9</td>
                        <td>18.3</td>
                    </tr>
                    <tr>
                        <td>L_CiteEval</td>
                        <td>Counting Stars</td>
                        <td>CP</td>
                        <td>Meta-Llama-3.1-8B-Instruct</td>
                        <td>15.5</td>
                        <td>16.9</td>
                    </tr>
                    <tr>
                        <td>L_CiteEval</td>
                        <td>Counting Stars</td>
                        <td>CR</td>
                        <td>Meta-Llama-3.1-8B-Instruct</td>
                        <td>18.3</td>
                        <td>18.3</td>
                    </tr>
                    <tr>
                        <td>L_CiteEval</td>
                        <td>Counting Stars</td>
                        <td>F1</td>
                        <td>Meta-Llama-3.1-8B-Instruct</td>
                        <td>15.8</td>
                        <td>19.2</td>
                    </tr>
                    <tr>
                        <td>L_CiteEval</td>
                        <td>Counting Stars</td>
                        <td>N</td>
                        <td>Meta-Llama-3.1-8B-Instruct</td>
                        <td>3.7</td>
                        <td>4.2</td>
                    </tr>
                    <tr>
                        <td>L_CiteEval</td>
                        <td>Counting Stars</td>
                        <td>Overall Average</td>
                        <td>Meta-Llama-3.1-8B-Instruct</td>
                        <td>27.7</td>
                        <td>25.5</td>
                    </tr>
                </tbody>
            </table>

            <!-- General Section -->
            <h2 id="LEval">1. LEval</h2>
            <h3>a. Computational Cost (Inference Time)</h3>
            <table class="evaluation-table">
                <thead>
                    <tr>
                        <th>Benchmark</th>
                        <th>Hardware Configuration</th>
                        <th>Inference Time (ALL tasks)</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>LEval</td>
                        <td>3090 (24GB) √ó 8</td>
                        <td>6:09:10</td>
                    </tr>
                    <tr>
                        <td>LEval</td>
                        <td>A100 (40GB) √ó 4</td>
                        <td>3:58:46</td>
                    </tr>
                    <tr>
                        <td>LEval</td>
                        <td>H20 (96GB) √ó 2</td>
                        <td>3:41:35</td>
                    </tr>
                </tbody>
            </table>

            <h3>b. Evaluation Results</h3>
            <table class="evaluation-table">
                <thead>
                    <tr>
                        <th>Benchmark</th>
                        <th>Subtask</th>
                        <th>Metrics</th>
                        <th>Model</th>
                        <th>Results (Evaluated)</th>
                        <th>Official Results (Reported in Paper)</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>LEval</td>
                        <td>TOEFL</td>
                        <td>exam</td>
                        <td>Meta-Llama-3.1-8B-Instruct</td>
                        <td>81.4</td>
                        <td>82.9</td>
                    </tr>
                    <tr>
                        <td>LEval</td>
                        <td>QuALITY</td>
                        <td>exam</td>
                        <td>Meta-Llama-3.1-8B-Instruct</td>
                        <td>63.3</td>
                        <td>64.9</td>
                    </tr>
                    <tr>
                        <td>LEval</td>
                        <td>Coursera</td>
                        <td>exam</td>
                        <td>Meta-Llama-3.1-8B-Instruct</td>
                        <td>54.9</td>
                        <td>53.8</td>
                    </tr>
                    <tr>
                        <td>LEval</td>
                        <td>SFiction</td>
                        <td>exact_match</td>
                        <td>Meta-Llama-3.1-8B-Instruct</td>
                        <td>78.9</td>
                        <td>69.5</td>
                    </tr>
                    <tr>
                        <td>LEval</td>
                        <td>GSM</td>
                        <td>exam</td>
                        <td>Meta-Llama-3.1-8B-Instruct</td>
                        <td>78.0</td>
                        <td>79.0</td>
                    </tr>
                    <tr>
                        <td>LEval</td>
                        <td>CodeU</td>
                        <td>exam</td>
                        <td>Meta-Llama-3.1-8B-Instruct</td>
                        <td>6.60</td>
                        <td>2.2</td>
                    </tr>
                </tbody>
            </table>

            <!-- LongBench Section -->
            <h2 id="LongBench">2. LongBench</h2>
            <h3>a. Computational Cost (Inference Time)</h3>
            <table class="evaluation-table">
                <thead>
                    <tr>
                        <th>Benchmark</th>
                        <th>Hardware Configuration</th>
                        <th>Inference Time (ALL tasks)</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>longbench</td>
                        <td>3090 (24GB) √ó 2</td>
                        <td>8:28:28</td>
                    </tr>
                    <tr>
                        <td>longbench</td>
                        <td>A100 (40GB) √ó 1</td>
                        <td>4:00:00</td>
                    </tr>
                    <tr>
                        <td>longbench</td>
                        <td>H20 (96GB) √ó 1</td>
                        <td>6:25:36</td>
                    </tr>
                </tbody>
            </table>

            <h3>b. Evaluation Results</h3>
            <table class="evaluation-table">
                <thead>
                    <tr>
                        <th>Benchmark</th>
                        <th>Subtask</th>
                        <th>Model</th>
                        <th>Results (Evaluated)</th>
                        <th>Official Results (Reported in Paper)</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>longbench</td>
                        <td>Single-Doc QA</td>
                        <td>ChatGLM2-6B</td>
                        <td>24.9</td>
                        <td>25.6</td>
                    </tr>
                    <tr>
                        <td>longbench</td>
                        <td>Multi-Doc QA</td>
                        <td>ChatGLM2-6B</td>
                        <td>15.3</td>
                        <td>16.2</td>
                    </tr>
                    <tr>
                        <td>longbench</td>
                        <td>Summarization</td>
                        <td>ChatGLM2-6B</td>
                        <td>21.5</td>
                        <td>21</td>
                    </tr>
                    <tr>
                        <td>longbench</td>
                        <td>Few-shot Learning</td>
                        <td>ChatGLM2-6B</td>
                        <td>38.2</td>
                        <td>41.3</td>
                    </tr>
                    <tr>
                        <td>longbench</td>
                        <td>Synthetic</td>
                        <td>ChatGLM2-6B</td>
                        <td>3.6</td>
                        <td>4.0</td>
                    </tr>
                    <tr>
                        <td>longbench</td>
                        <td>Code</td>
                        <td>ChatGLM2-6B</td>
                        <td>45.7</td>
                        <td>46.1</td>
                    </tr>
                    <tr>
                        <td>longbench</td>
                        <td>Overall Average</td>
                        <td>ChatGLM2-6B</td>
                        <td>24.9</td>
                        <td>25.7</td>
                    </tr>
                </tbody>
            </table>

            <!-- ‰ª•‰∏ãÊåâÁõ∏ÂêåÁªìÊûÑÁªßÁª≠Ê∑ªÂä† LooGLE„ÄÅRULER„ÄÅLongWriter Á≠âÂâ©‰ΩôÂü∫ÂáÜÊµãËØïÂÜÖÂÆπ -->

            <!-- LooGLE Section -->
            <h2 id="LooGLE">3. LooGLE</h2>
            <h3>a. Computational Cost (Inference Time)</h3>
            <table class="evaluation-table">
                <thead>
                    <tr>
                        <th>Benchmark</th>
                        <th>Hardware Configuration</th>
                        <th>Inference Time (ALL tasks)</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>LooGLE</td>
                        <td>3090 (24GB) √ó 1</td>
                        <td>1:54:32</td>
                    </tr>
                    <tr>
                        <td>LooGLE</td>
                        <td>A100 (40G) √ó 1</td>
                        <td>1:48:46</td>
                    </tr>
                    <tr>
                        <td>LooGLE</td>
                        <td>H20 (96GB) √ó 1</td>
                        <td>2:16:48</td>
                    </tr>
                </tbody>
            </table>

            <h3>b. Evaluation Results</h3>
            <table class="evaluation-table">
                <thead>
                    <tr>
                        <th>Benchmark</th>
                        <th>Subtask</th>
                        <th>Metrics</th>
                        <th>Model</th>
                        <th>Results (Evaluated)</th>
                        <th>Official Results (Reported in Paper)</th>
                    </tr>
                </thead>
                <tbody>
                    <!-- ÊåâMarkdownÂÜÖÂÆπÈÄêË°åÊ∑ªÂä†LooGLEËØÑ‰º∞ÁªìÊûú -->
                    <tr>
                        <td>LooGLE</td>
                        <td>summarization</td>
                        <td>Bleu1</td>
                        <td>chatglm2-6b-32k</td>
                        <td>20.9</td>
                        <td>4.2</td>
                    </tr>
                    <!-- ÂÖ∂‰ªñË°å‰æùÊ¨°Ê∑ªÂä† -->
                </tbody>
            </table>

            <!-- Ê≠§Â§ÑÁúÅÁï•ÈáçÂ§çÁªìÊûÑÔºåÊåâÁõ∏ÂêåÊñπÂºèÊ∑ªÂä†RULER„ÄÅLongWriter„ÄÅbabilongÁ≠âÂâ©‰ΩôÈÉ®ÂàÜ -->

        </section>

                
        <section class="citation">
            <h2><i class="fas fa-quote-right"></i> BibTeX</h2>
            <div class="citation-container">
                <button class="copy-button" onclick="copyBibTeX()">
                    <i class="fas fa-copy"></i> Copy
                </button>
                <pre id="bibtex">@misc{,
    title={LOOM-Scope: LOng-cOntext Model evaluation framework }, 
    author={ },
    year={2025},
    eprint={2504.19093},
    archivePrefix={arXiv},
    primaryClass={cs.CR},
    url={}, 
}</pre>
            </div>
        </section>

        <script>
        function copyBibTeX() {
            const bibtex = document.getElementById('bibtex').textContent;
            navigator.clipboard.writeText(bibtex).then(() => {
                const button = document.querySelector('.copy-button');
                const originalContent = button.innerHTML;
                button.innerHTML = '<i class="fas fa-check"></i> Copied!';
                setTimeout(() => {
                    button.innerHTML = originalContent;
                }, 2000);
            });
        }
        </script>
    </main>
</body>
</html> 